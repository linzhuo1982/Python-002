学习笔记
终于可以系统性的进行一些学习了，以前都是因为工作需要东拼西凑的边学边写，很多知识和技巧都不够扎实。这次公司给予的机会一定要好好把握！
第一周的课程让我一下学到了很多新东西，还需要对这些内容进一步消化，再次先记录下来。
git工具：初始化、追踪、提交、推代码、分支切换（有时候会在分支里修改并保存了代码，在切换时就会失败。还不知道如何改善此问题）
github:fork别人的项目，自建项目

作业一：
对requests、pandas、bs4的库和html的结构有了较多的了解，并通过作业的编写对库的使用有了实操认识。
总结一下都有那些坑：
1-使用fand_all等方法，在里面配置的参数，属性的参数中，如果写错了会报错，但是基本看不出来。就好比“attrs={'class':'movie-hover-info'}”中的class，被我写成calss。导致很长时间的调试毫无头绪，各个数据点都加了print来看进展。
2-useragent使用多过，请求到的html都是乱码，更换其他useragent值能恢复正常的请求。
3-然后就是要审题清楚，之前以为去猫眼榜单里爬前十，原来是在指定的地址取前十。浪费了不少时间。

拎重点：
1-百度了一下bs4库现好像不是主流的爬取库了，但是还是需要对基础的功能和功能特性加强了解。过程中，在源码里找到并标签div后，如何提取下层中<span>,<div>等不同层级的内容时也浪费了很多时间，后来多方查看发现可以在find的结果中加上中括号指定第几个标签，才解决了问题。


作业二：
重点学习，了解了主流爬虫库scrapy，也对scrapy的整体框架和基本功能有了全面认识。

总结一下，学习的经验：
1-scrapy的配置和启动与平常python的方式不同，需要在终端中按特定要求进行设置和运行；
2-了解到，scrapy抓取的数据是由item类作为简单容器，提供类似字典的API，再由管道pipelines类抽取项目存到介质里。
3-另外，要实现完整的爬取，还需要对setting.py里的配置进一步的去学习。

整体总结：
1-在html的层级定位中还是有点迷糊，需要反复调试不停的参考，才能让for in遍历到想要的内容。
2-scrapy里传递到pipelines的数据，为什么直接向pandas里DataFrame里反馈item不行，只显示标题，但在pipeline里将接收的放到列表里就可以。还需要在琢磨琢磨